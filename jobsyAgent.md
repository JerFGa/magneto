# IntegraciÃ³n de Ollama con Llama 3.1 - Simulador de Entrevistas

## ğŸ“‹ DescripciÃ³n

La aplicaciÃ³n ahora utiliza **Ollama con Llama 3.1** para generar preguntas de entrevista personalizadas y dinÃ¡micas basadas en el perfil del usuario.

## ğŸš€ CÃ³mo funciona

### 1. Backend (server.js)
- Se agregÃ³ el paquete `ollama` para comunicarse con la instancia local de Ollama
- Nuevo endpoint: `POST /api/generate-question`
- El endpoint recibe:
  - **userProfile**: Datos del usuario (experiencia, sector objetivo, rol, fortalezas, mejoras)
  - **questionNumber**: NÃºmero de la pregunta actual
  - **totalQuestions**: Total de preguntas en la entrevista
  - **previousQuestions**: Preguntas ya generadas (para evitar repeticiÃ³n)

### 2. Frontend (InterviewSimulator.tsx)
- Al iniciar el simulador, se genera la primera pregunta automÃ¡ticamente
- Cada vez que el usuario avanza, se genera la siguiente pregunta
- Muestra un indicador de carga mientras se genera la pregunta
- Si Ollama no estÃ¡ disponible, usa preguntas de respaldo predefinidas

## ğŸ¯ PersonalizaciÃ³n de Preguntas

Las preguntas generadas por Llama 3.1 son personalizadas segÃºn:
- âœ… Experiencia laboral del usuario
- âœ… Sector objetivo (ej: tecnologÃ­a, finanzas, marketing)
- âœ… Puesto objetivo (ej: desarrollador, gerente, analista)
- âœ… Fortalezas identificadas
- âœ… Ãreas de mejora
- âœ… Contexto de preguntas anteriores

## ğŸ› ï¸ Requisitos

### Ollama debe estar instalado y corriendo
```bash
# Verificar que Ollama estÃ¡ corriendo
curl http://localhost:11434/api/tags

# Si no estÃ¡ corriendo, iniciarlo
ollama serve
```

### Modelo Llama 3.1 descargado
```bash
# Descargar el modelo si no lo tienes
ollama pull llama3.1

# Verificar que el modelo estÃ¡ disponible
ollama list
```

## ğŸ”§ ConfiguraciÃ³n

El servidor se conecta a Ollama en: `http://localhost:11434` (puerto por defecto)

Si necesitas cambiar el puerto, edita la lÃ­nea en `server.js`:
```javascript
const ollama = new Ollama({ host: 'http://localhost:11434' });
```

## ğŸ§ª Modo de Respaldo

Si Ollama no estÃ¡ disponible o hay un error:
- âš ï¸ Se mostrarÃ¡n preguntas predefinidas de respaldo
- ğŸ”„ El usuario puede continuar con la entrevista sin interrupciones
- ğŸ·ï¸ Las preguntas se marcan como "Pregunta estÃ¡ndar" en lugar de "Generada por IA"

## ğŸ“ Ejemplo de Prompt

```
Eres un experto entrevistador de recursos humanos. Genera UNA pregunta de entrevista laboral relevante y profesional.

Perfil del candidato:
- Experiencia: 3 aÃ±os como desarrollador web
- Sector objetivo: TecnologÃ­a
- Puesto objetivo: Desarrollador Full Stack Senior
- Fortalezas: React, Node.js, trabajo en equipo
- Ãreas de mejora: Liderazgo de proyectos

Preguntas anteriores (evita repetir temas similares):
Â¿Puedes contarme sobre tu experiencia mÃ¡s relevante con React?

Esta es la pregunta 2 de 5 en la entrevista.

INSTRUCCIONES:
- Genera SOLO la pregunta, sin numeraciÃ³n ni formato adicional
- La pregunta debe ser clara, profesional y relevante para el perfil
- Debe ser apropiada para una entrevista real
- No incluyas mÃºltiples preguntas
- No agregues explicaciones ni contexto adicional

Pregunta:
```

## ğŸ¨ Interfaz de Usuario

### Indicadores visuales:
- ğŸ”„ **Spinner de carga**: "Generando pregunta personalizada con IA..."
- ğŸ¤– **Badge "Generada por IA"**: Pregunta creada por Llama 3.1
- ğŸ“„ **Badge "Pregunta estÃ¡ndar"**: Pregunta de respaldo

## ğŸ› SoluciÃ³n de Problemas

### Error: "Cannot find module 'ollama'"
```bash
npm install ollama
```

### Error: "ECONNREFUSED localhost:11434"
```bash
# AsegÃºrate de que Ollama estÃ¡ corriendo
ollama serve
```

### Error: "model 'llama3.1' not found"
```bash
ollama pull llama3.1
```

### Las preguntas son genÃ©ricas
- Verifica que el usuario haya completado su perfil en la aplicaciÃ³n
- Los datos del perfil se usan para personalizar las preguntas

## ğŸ“Š Ventajas

1. **Preguntas Personalizadas**: Adaptadas al perfil especÃ­fico del candidato
2. **No Repetitivas**: El sistema evita hacer preguntas similares
3. **Contextualizadas**: Considera el progreso de la entrevista
4. **Offline**: No requiere APIs externas o conexiÃ³n a internet
5. **Privacidad**: Los datos se procesan localmente
6. **Escalable**: FÃ¡cil de extender con mÃ¡s modelos o personalizaciÃ³n

## ğŸš€ PrÃ³ximas Mejoras

- [ ] AnÃ¡lisis de respuestas con IA
- [ ] Feedback personalizado por pregunta
- [ ] Diferentes niveles de dificultad dinÃ¡micos
- [ ] Seguimiento de temas especÃ­ficos segÃºn las respuestas
- [ ] IntegraciÃ³n con mÃ¡s modelos de Ollama (mistral, codellama, etc.)

## ğŸ“ Soporte

Si encuentras problemas con la integraciÃ³n:
1. Verifica que Ollama estÃ© corriendo: `ollama serve`
2. Verifica que llama3.1 estÃ© instalado: `ollama list`
3. Revisa los logs del servidor en la consola
4. Verifica la consola del navegador para errores de frontend
